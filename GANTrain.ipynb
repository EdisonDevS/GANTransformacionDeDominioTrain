{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"facades_256x256.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"U1GMbCMWEjDC","colab_type":"code","colab":{}},"source":["!pip uninstall scipy\n","!pip install scipy==1.0.0\n","!pip install git+https://www.github.com/keras-team/keras-contrib.git"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ifs1IUK5ErK1","colab_type":"code","cellView":"both","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"or_70b7lEtpU","colab_type":"code","colab":{}},"source":["!ls \"/content/drive/My Drive/Colab Notebooks\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3PtSqkdXE_yI","colab_type":"code","colab":{}},"source":["import scipy\n","from glob import glob\n","import numpy as np\n","from __future__ import print_function, division\n","import scipy.misc\n","\n","\n","from keras.datasets import mnist\n","from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\n","from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n","from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n","from keras.layers.advanced_activations import LeakyReLU\n","from keras.layers.convolutional import UpSampling2D, Conv2D\n","from keras.models import Sequential, Model\n","from keras.optimizers import Adam\n","import datetime\n","import matplotlib.pyplot as plt\n","import sys\n","import numpy as np\n","import os"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XupK06vcFDil","colab_type":"code","colab":{}},"source":["class DataLoader():\n","    def __init__(self, dataset_name, img_res=(128, 128)):\n","        self.dataset_name = dataset_name\n","        self.img_res = img_res\n","\n","    def load_data(self, domain, batch_size=1, is_testing=False):\n","        data_type = \"train%s\" % domain if not is_testing else \"test%s\" % domain\n","        path = glob('/content/drive/My Drive/Colab Notebooks/datasets/%s/%s/*' % (self.dataset_name, data_type))\n","\n","        batch_images = np.random.choice(path, size=batch_size)\n","\n","        imgs = []\n","        for img_path in batch_images:\n","            img = self.imread(img_path)\n","            if not is_testing:\n","                img = scipy.misc.imresize(img, self.img_res)\n","\n","                if np.random.random() > 0.5:\n","                    img = np.fliplr(img)\n","            else:\n","                img = scipy.misc.imresize(img, self.img_res)\n","            imgs.append(img)\n","\n","        imgs = np.array(imgs)/127.5 - 1.\n","\n","        return imgs\n","\n","    def load_batch(self, batch_size=1, is_testing=False):\n","        data_type = \"train\" if not is_testing else \"val\"\n","        path_A = glob('/content/drive/My Drive/Colab Notebooks/datasets/%s/%sA/*' % (self.dataset_name, data_type))\n","        path_B = glob('/content/drive/My Drive/Colab Notebooks/datasets/%s/%sB/*' % (self.dataset_name, data_type))\n","\n","        self.n_batches = int(min(len(path_A), len(path_B)) / batch_size)\n","        total_samples = self.n_batches * batch_size\n","\n","        # Sample n_batches * batch_size from each path list so that model sees all\n","        # samples from both domains\n","        path_A = np.random.choice(path_A, total_samples, replace=False)\n","        path_B = np.random.choice(path_B, total_samples, replace=False)\n","\n","        for i in range(self.n_batches-1):\n","            batch_A = path_A[i*batch_size:(i+1)*batch_size]\n","            batch_B = path_B[i*batch_size:(i+1)*batch_size]\n","            imgs_A, imgs_B = [], []\n","            for img_A, img_B in zip(batch_A, batch_B):\n","                img_A = self.imread(img_A)\n","                img_B = self.imread(img_B)\n","\n","                img_A = scipy.misc.imresize(img_A, self.img_res)\n","                img_B = scipy.misc.imresize(img_B, self.img_res)\n","\n","                if not is_testing and np.random.random() > 0.5:\n","                        img_A = np.fliplr(img_A)\n","                        img_B = np.fliplr(img_B)\n","\n","                imgs_A.append(img_A)\n","                imgs_B.append(img_B)\n","\n","            imgs_A = np.array(imgs_A)/127.5 - 1.\n","            imgs_B = np.array(imgs_B)/127.5 - 1.\n","\n","            yield imgs_A, imgs_B\n","\n","    def load_img(self, path):\n","        img = self.imread(path)\n","        img = scipy.misc.imresize(img, self.img_res)\n","        img = img/127.5 - 1.\n","        return img[np.newaxis, :, :, :]\n","\n","    def imread(self, path):\n","        return scipy.misc.imread(path, mode='RGB').astype(np.float)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sEgrRZiIFIIs","colab_type":"code","colab":{}},"source":["class CycleGAN():\n","    def __init__(self):\n","        # Input shape\n","        self.img_rows = 256\n","        self.img_cols = 256\n","        self.channels = 3\n","        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n","\n","        # Configure data loader\n","        self.dataset_name = 'vangogh2photo'\n","        self.data_loader = DataLoader(dataset_name=self.dataset_name,\n","                                      img_res=(self.img_rows, self.img_cols))\n","\n","\n","        # Calculate output shape of D\n","        patch = int(self.img_rows / 2**4)\n","        self.disc_patch = (patch, patch, 1)\n","\n","        # Number of filters in the first layer of G and D\n","        self.gf = 32\n","        self.df = 64\n","\n","        # Loss weights\n","        self.lambda_cycle = 10.0                    # Cycle-consistency loss\n","        self.lambda_id = 0.1 * self.lambda_cycle    # Identity loss\n","\n","        optimizer = Adam(0.0002, 0.5)\n","\n","        # Build and compile the discriminators\n","        self.d_A = self.build_discriminator()\n","        self.d_B = self.build_discriminator()\n","        self.d_A.compile(loss='mse',\n","            optimizer=optimizer,\n","            metrics=['accuracy'])\n","        self.d_B.compile(loss='mse',\n","            optimizer=optimizer,\n","            metrics=['accuracy'])\n","\n","        #-------------------------\n","        # Construct Computational\n","        #   Graph of Generators\n","        #-------------------------\n","\n","        # Build the generators\n","        self.g_AB = self.build_generator()\n","        self.g_BA = self.build_generator()\n","\n","        # Input images from both domains\n","        img_A = Input(shape=self.img_shape)\n","        img_B = Input(shape=self.img_shape)\n","\n","        # Translate images to the other domain\n","        fake_B = self.g_AB(img_A)\n","        fake_A = self.g_BA(img_B)\n","        # Translate images back to original domain\n","        reconstr_A = self.g_BA(fake_B)\n","        reconstr_B = self.g_AB(fake_A)\n","        # Identity mapping of images\n","        img_A_id = self.g_BA(img_A)\n","        img_B_id = self.g_AB(img_B)\n","\n","        # For the combined model we will only train the generators\n","        self.d_A.trainable = False\n","        self.d_B.trainable = False\n","\n","        # Discriminators determines validity of translated images\n","        valid_A = self.d_A(fake_A)\n","        valid_B = self.d_B(fake_B)\n","\n","        # Combined model trains generators to fool discriminators\n","        self.combined = Model(inputs=[img_A, img_B],\n","                              outputs=[ valid_A, valid_B,\n","                                        reconstr_A, reconstr_B,\n","                                        img_A_id, img_B_id ])\n","        self.combined.compile(loss=['mse', 'mse',\n","                                    'mae', 'mae',\n","                                    'mae', 'mae'],\n","                            loss_weights=[  1, 1,\n","                                            self.lambda_cycle, self.lambda_cycle,\n","                                            self.lambda_id, self.lambda_id ],\n","                            optimizer=optimizer)\n","\n","    def build_generator(self):\n","        \"\"\"U-Net Generator\"\"\"\n","\n","        def conv2d(layer_input, filters, f_size=4):\n","            \"\"\"Layers used during downsampling\"\"\"\n","            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n","            d = LeakyReLU(alpha=0.2)(d)\n","            d = InstanceNormalization()(d)\n","            return d\n","\n","        def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n","            \"\"\"Layers used during upsampling\"\"\"\n","            u = UpSampling2D(size=2)(layer_input)\n","            u = Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n","            if dropout_rate:\n","                u = Dropout(dropout_rate)(u)\n","            u = InstanceNormalization()(u)\n","            u = Concatenate()([u, skip_input])\n","            return u\n","\n","        # Image input\n","        d0 = Input(shape=self.img_shape)\n","\n","        # Downsampling\n","        d1 = conv2d(d0, self.gf)\n","        d2 = conv2d(d1, self.gf*2)\n","        d3 = conv2d(d2, self.gf*4)\n","        d4 = conv2d(d3, self.gf*8)\n","\n","        # Upsampling\n","        u1 = deconv2d(d4, d3, self.gf*4)\n","        u2 = deconv2d(u1, d2, self.gf*2)\n","        u3 = deconv2d(u2, d1, self.gf)\n","\n","        u4 = UpSampling2D(size=2)(u3)\n","        output_img = Conv2D(self.channels, kernel_size=4, strides=1, padding='same', activation='tanh')(u4)\n","\n","        return Model(d0, output_img)\n","\n","    def build_discriminator(self):\n","\n","        def d_layer(layer_input, filters, f_size=4, normalization=True):\n","            \"\"\"Discriminator layer\"\"\"\n","            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n","            d = LeakyReLU(alpha=0.2)(d)\n","            if normalization:\n","                d = InstanceNormalization()(d)\n","            return d\n","\n","        img = Input(shape=self.img_shape)\n","\n","        d1 = d_layer(img, self.df, normalization=False)\n","        d2 = d_layer(d1, self.df*2)\n","        d3 = d_layer(d2, self.df*4)\n","        d4 = d_layer(d3, self.df*8)\n","\n","        validity = Conv2D(1, kernel_size=4, strides=1, padding='same')(d4)\n","\n","        return Model(img, validity)\n","\n","    def train(self, epochs, batch_size=1, sample_interval=50):\n","\n","        start_time = datetime.datetime.now()\n","\n","        print(\"entrenamiento empezado time: %s\" % start_time)\n","\n","        # Adversarial loss ground truths\n","        valid = np.ones((batch_size,) + self.disc_patch)\n","        fake = np.zeros((batch_size,) + self.disc_patch)\n","\n","        for epoch in range(epochs):\n","            for batch_i, (imgs_A, imgs_B) in enumerate(self.data_loader.load_batch(batch_size)):\n","\n","                # ----------------------\n","                #  Train Discriminators\n","                # ----------------------\n","\n","                # Translate images to opposite domain\n","                fake_B = self.g_AB.predict(imgs_A)\n","                fake_A = self.g_BA.predict(imgs_B)\n","\n","                # Train the discriminators (original images = real / translated = Fake)\n","                dA_loss_real = self.d_A.train_on_batch(imgs_A, valid)\n","                dA_loss_fake = self.d_A.train_on_batch(fake_A, fake)\n","                dA_loss = 0.5 * np.add(dA_loss_real, dA_loss_fake)\n","\n","                dB_loss_real = self.d_B.train_on_batch(imgs_B, valid)\n","                dB_loss_fake = self.d_B.train_on_batch(fake_B, fake)\n","                dB_loss = 0.5 * np.add(dB_loss_real, dB_loss_fake)\n","\n","                # Total disciminator loss\n","                d_loss = 0.5 * np.add(dA_loss, dB_loss)\n","\n","\n","                # ------------------\n","                #  Train Generators\n","                # ------------------\n","\n","                # Train the generators\n","                g_loss = self.combined.train_on_batch([imgs_A, imgs_B],\n","                                                        [valid, valid,\n","                                                        imgs_A, imgs_B,\n","                                                        imgs_A, imgs_B])\n","\n","                \n","\n","                # Plot the progress\n","                #print (\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f, acc: %3d%%] [G loss: %05f, adv: %05f, recon: %05f, id: %05f] time: %s \" \\\n","                #                                                        % ( epoch, epochs,\n","                #                                                            batch_i, self.data_loader.n_batches,\n","                #                                                            d_loss[0], 100*d_loss[1],\n","                #                                                            g_loss[0],\n","                #                                                            np.mean(g_loss[1:3]),\n","                #                                                            np.mean(g_loss[3:5]),\n","                #                                                            np.mean(g_loss[5:6]),\n","                #                                                            elapsed_time))\n","\n","                # If at save interval => save generated image samples\n","                if batch_i % sample_interval == 0:\n","                    self.sample_images(epoch, batch_i)\n","\n","        elapsed_time = datetime.datetime.now() - start_time\n","                    \n","        print(\"entrenamiento finalizado time: %s\" % elapsed_time)\n","        \n","        \n","    def sample_images(self, epoch, batch_i):\n","        os.makedirs('/content/drive/My Drive/Colab Notebooks/images', exist_ok=True)\n","        os.makedirs('/content/drive/My Drive/Colab Notebooks/images/%s 256' % self.dataset_name, exist_ok=True)\n","        r, c = 2, 3\n","\n","        imgs_A = self.data_loader.load_data(domain=\"A\", batch_size=1, is_testing=True)\n","        imgs_B = self.data_loader.load_data(domain=\"B\", batch_size=1, is_testing=True)\n","\n","        # Demo (for GIF)\n","        #imgs_A = self.data_loader.load_img('datasets/apple2orange/testA/n07740461_1541.jpg')\n","        #imgs_B = self.data_loader.load_img('datasets/apple2orange/testB/n07749192_4241.jpg')\n","\n","        # Translate images to the other domain\n","        fake_B = self.g_AB.predict(imgs_A)\n","        fake_A = self.g_BA.predict(imgs_B)\n","        # Translate back to original domain\n","        reconstr_A = self.g_BA.predict(fake_B)\n","        reconstr_B = self.g_AB.predict(fake_A)\n","\n","        gen_imgs = np.concatenate([imgs_A, fake_B, reconstr_A, imgs_B, fake_A, reconstr_B])\n","\n","        # Rescale images 0 - 1\n","        gen_imgs = 0.5 * gen_imgs + 0.5\n","\n","        titles = ['Original', 'Translated', 'Reconstructed']\n","        fig, axs = plt.subplots(r, c)\n","        cnt = 0\n","        for i in range(r):\n","            for j in range(c):\n","                axs[i,j].imshow(gen_imgs[cnt])\n","                axs[i, j].set_title(titles[j])\n","                axs[i,j].axis('off')\n","                cnt += 1\n","        fig.savefig(\"/content/drive/My Drive/Colab Notebooks/images/%s 256/%d_%d.png\" % (self.dataset_name, epoch, batch_i))\n","        plt.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pJtGuc38FOqW","colab_type":"code","outputId":"ed1bfd57-c5f0-4374-9e9c-2daef3113fbd","executionInfo":{"status":"ok","timestamp":1572356845010,"user_tz":300,"elapsed":34170611,"user":{"displayName":"Jhon Edison Giraldo Mejía","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBuuom8zV0azZzpt4SYrGiIM_e3Rks6uVST0WyIgQ=s64","userId":"17349958004177838144"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["gan = CycleGAN()\n","gan.train(epochs=200, batch_size=1, sample_interval=200)\n","\n","#guardando los pesos\n","os.makedirs('/content/drive/My Drive/Colab Notebooks/weights', exist_ok=True)\n","os.makedirs('/content/drive/My Drive/Colab Notebooks/weights/%s 256' % gan.dataset_name, exist_ok=True)\n","\n","gan.d_A.save(\"/content/drive/My Drive/Colab Notebooks/weights/%s 256/d_A_complete_model_256.h5\" % gan.dataset_name)\n","gan.d_B.save(\"/content/drive/My Drive/Colab Notebooks/weights/%s 256/d_B_complete_model_256.h5\" % gan.dataset_name)\n","gan.g_AB.save(\"/content/drive/My Drive/Colab Notebooks/weights/%s 256/g_AB_complete_model_256.h5\" % gan.dataset_name)\n","gan.g_BA.save(\"/content/drive/My Drive/Colab Notebooks/weights/%s 256/g_BA_complete_model_256.h5\" % gan.dataset_name)\n","gan.combined.save(\"/content/drive/My Drive/Colab Notebooks/weights/%s 256/combined_complete_model_256.h5\" % gan.dataset_name)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2239: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n","\n","entrenamiento empezado time: 2019-11-26 23:27:58.841212\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:71: DeprecationWarning: `imread` is deprecated!\n","`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n","Use ``imageio.imread`` instead.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:49: DeprecationWarning: `imresize` is deprecated!\n","`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n","Use ``skimage.transform.resize`` instead.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:50: DeprecationWarning: `imresize` is deprecated!\n","`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n","Use ``skimage.transform.resize`` instead.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n","  'Discrepancy between trainable weights and collected trainable'\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n","  'Discrepancy between trainable weights and collected trainable'\n","/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n","  'Discrepancy between trainable weights and collected trainable'\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:71: DeprecationWarning: `imread` is deprecated!\n","`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n","Use ``imageio.imread`` instead.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: DeprecationWarning: `imresize` is deprecated!\n","`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n","Use ``skimage.transform.resize`` instead.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:49: DeprecationWarning: `imresize` is deprecated!\n","`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n","Use ``skimage.transform.resize`` instead.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:50: DeprecationWarning: `imresize` is deprecated!\n","`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n","Use ``skimage.transform.resize`` instead.\n","/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n","  'Discrepancy between trainable weights and collected trainable'\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"aMAbkxQfFkme","colab_type":"code","outputId":"cd940054-9109-466c-92da-054f8422d735","executionInfo":{"status":"ok","timestamp":1572358120184,"user_tz":300,"elapsed":35081,"user":{"displayName":"Jhon Edison Giraldo Mejía","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBuuom8zV0azZzpt4SYrGiIM_e3Rks6uVST0WyIgQ=s64","userId":"17349958004177838144"}},"colab":{"base_uri":"https://localhost:8080/","height":253}},"source":["from keras.models import load_model\n","import keras_contrib\n","#cargando los modelos entrenados\n","\n","gan.g_AB = load_model(\"/content/drive/My Drive/Colab Notebooks/weights/%s 256/g_AB_complete_model_256.h5\" % gan.dataset_name, custom_objects={'InstanceNormalization':keras_contrib.layers.InstanceNormalization})\n","gan.g_BA = load_model(\"/content/drive/My Drive/Colab Notebooks/weights/%s 256/g_BA_complete_model_256.h5\" % gan.dataset_name, custom_objects={'InstanceNormalization':keras_contrib.layers.InstanceNormalization})\n","gan.d_A = load_model(\"/content/drive/My Drive/Colab Notebooks/weights/%s 256/d_A_complete_model_256.h5\" % gan.dataset_name, custom_objects={'InstanceNormalization':keras_contrib.layers.InstanceNormalization})\n","gan.d_B = load_model(\"/content/drive/My Drive/Colab Notebooks/weights/%s 256/d_B_complete_model_256.h5\" % gan.dataset_name, custom_objects={'InstanceNormalization':keras_contrib.layers.InstanceNormalization})\n","\n","gan.sample_images(1,1)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:310: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n","  warnings.warn('No training configuration found in save file: '\n","/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:350: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n","  warnings.warn('Error in loading the saved optimizer '\n","/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:350: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n","  warnings.warn('Error in loading the saved optimizer '\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:71: DeprecationWarning: `imread` is deprecated!\n","`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n","Use ``imageio.imread`` instead.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: DeprecationWarning: `imresize` is deprecated!\n","`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n","Use ``skimage.transform.resize`` instead.\n"],"name":"stderr"}]}]}